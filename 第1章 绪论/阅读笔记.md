### 1.
P3 最后说到:
> 一般而言，训练样本越多，我们得到的关于（分布）D的信息越多，这样就越有可能获得具有强泛化能力的模拟

那么问题是，多少信息算是足够，泛化能力的强弱用什么来衡量呢？信息的多少是根据样本占总体的比例来衡量的吗？<br>
### 2.
P4 中间说到:
> 概念学习技术目前研究、应用都比较少，因为要学得泛化性能好且语义明确的概念实在太困难了<br>

结合人类学习概念的过程，概念是抽象的，如果能具象化，如指代具体物品的名词，相对会快很多。<br>
但是如果是抽象名词，那么这个过程显然会很缓慢。这个学习的过程更像是对抽象名词逐渐描绘其轮廓，描绘的面更加细致广阔以后，才能逐渐显示其真正面貌。<br>\
那么，概念学习是这样一个过程吗？<br>
### 3.
P5左边注释说到`噪声`这个概念，有的时候噪声是需要我们去除的，作为干扰，但是有的时候噪声是否就是我们需要关注的对象？<br>
这也是机器学习关注的一个方面吗？<br>
### 4.
P7提到了奥卡姆剃刀(Occam's razor)原则，即“若有多个假设与观察一致，则选择最简单的那个”，然而自然科学中好像并不是这样的<br>
比如牛顿定律只是相对论在低速时空的简单描述，真实的情况用相对论来描述显然更精确，在什么情况下应该用更简单的原则，在什么情况下又应该用更复杂的原则呢？<br>
### 5.
P9有这样一句话:
> 所以，NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义

结合不知道我在哪里瞥到的关于某顶会的获奖论文（该论文论点好像是使用算法识别一个扔枕头的视频中枕头的飞行路线，简单算法难以识别，但是如果引入了人类的认识——一般情况下，扔的结果是一条抛物线，那么算法就可以很快实现），引入既有的经验后，就可以更快拟合事实数据。<br>
那么我们该怎么结合具体？如何能够让机器具有可习得性质的记忆？如何让机器保留其他地方获取的经验并应用在具体情境中？如何连接不同机器学习算法在不同情境下的学习成果并相互影响形成更加完备的模型？
### 6.
